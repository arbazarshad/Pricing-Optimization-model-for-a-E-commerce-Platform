{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d87a4b2",
   "metadata": {},
   "source": [
    "# Approach 1 - Simple conditional modelling for strict rules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dba6d9d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             sku category    mrp      date  qty_sold  revenue  orders  \\\n",
      "0    AN201-RED-M   Bottom  229.0  04-07-22         1    229.0       1   \n",
      "1    AN201-RED-M   Bottom  229.0  05-01-22         1    229.0       1   \n",
      "2   AN201-RED-XL   Bottom  301.0  06-24-22         1    301.0       1   \n",
      "3   AN201-RED-XL   Bottom  301.0  06-26-22         1    301.0       1   \n",
      "4  AN201-RED-XXL   Bottom  229.0  05-03-22         1    229.0       1   \n",
      "\n",
      "   promo_qty  b2b_qty  non_b2b_qty  ...  intransit  returned_to_seller  \\\n",
      "0          1        0            1  ...          1                   0   \n",
      "1          1        0            1  ...          0                   0   \n",
      "2          1        0            1  ...          1                   0   \n",
      "3          0        0            1  ...          1                   0   \n",
      "4          1        0            1  ...          0                   0   \n",
      "\n",
      "   rejected_by_buyer  lost_in_transit  returning_to_seller  pending  \\\n",
      "0                  0                0                    0        0   \n",
      "1                  0                0                    0        0   \n",
      "2                  0                0                    0        0   \n",
      "3                  0                0                    0        0   \n",
      "4                  0                0                    0        0   \n",
      "\n",
      "   waiting_for_pickup  damaged  shipping  other  \n",
      "0                   0        0         0      0  \n",
      "1                   0        0         0      0  \n",
      "2                   0        0         0      0  \n",
      "3                   0        0         0      0  \n",
      "4                   0        0         0      0  \n",
      "\n",
      "[5 rows x 22 columns]\n",
      "(75016, 22)\n",
      "             sku category      date    mrp  qty_sold  promo_share  b2b_share  \\\n",
      "0    AN201-RED-M   Bottom  04-07-22  229.0         1          1.0        0.0   \n",
      "1    AN201-RED-M   Bottom  05-01-22  229.0         1          1.0        0.0   \n",
      "2   AN201-RED-XL   Bottom  06-24-22  301.0         1          1.0        0.0   \n",
      "3   AN201-RED-XL   Bottom  06-26-22  301.0         1          0.0        0.0   \n",
      "4  AN201-RED-XXL   Bottom  05-03-22  229.0         1          1.0        0.0   \n",
      "\n",
      "   suggested_mrp  \n",
      "0         206.10  \n",
      "1         206.10  \n",
      "2         270.90  \n",
      "3         316.05  \n",
      "4         206.10  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Load ---\n",
    "df = pd.read_csv(\"C:/Users/MOHAMMED ARBAZ/Downloads/Amazon Sale Report.csv/Amazon Sale Report.csv\")\n",
    "\n",
    "# --- Normalize column names ---\n",
    "def normalize_col(c):\n",
    "    return (\n",
    "        str(c).strip()\n",
    "        .replace(\"\\xa0\", \" \")\n",
    "        .replace(\"\\u200b\", \"\")\n",
    "        .strip()\n",
    "        .lower()\n",
    "        .replace(\" \", \"_\")\n",
    "        .replace(\"-\", \"_\")\n",
    "    )\n",
    "\n",
    "df.columns = [normalize_col(c) for c in df.columns]\n",
    "\n",
    "# --- Flag promo rows (where promotion_ids is NOT null/blank) ---\n",
    "df[\"promo_flag\"] = df[\"promotion_ids\"].notna().astype(int) if \"promotion_ids\" in df.columns else 0\n",
    "\n",
    "# --- Ensure numeric fields ---\n",
    "df[\"qty\"] = pd.to_numeric(df[\"qty\"], errors=\"coerce\")\n",
    "df[\"amount\"] = pd.to_numeric(df[\"amount\"], errors=\"coerce\")\n",
    "\n",
    "# --- Clean Status and Remove Cancelled ---\n",
    "if \"status\" in df.columns:\n",
    "    status_clean = df[\"status\"].astype(str).str.strip().str.lower()\n",
    "    df = df[~status_clean.isin([\"cancelled\", \"canceled\"])].copy()\n",
    "\n",
    "# Drop rows with invalid qty or amount\n",
    "df = df.dropna(subset=[\"amount\"])\n",
    "df = df[df[\"qty\"] > 0].copy()\n",
    "\n",
    "# --- Compute MRP ---\n",
    "df[\"mrp\"] = df[\"amount\"] / df[\"qty\"]\n",
    "df = df[np.isfinite(df[\"mrp\"]) & (df[\"mrp\"] > 0)].copy()\n",
    "\n",
    "# --- Drop null SKU or Category ---\n",
    "df = df.dropna(subset=[\"sku\", \"category\"]).copy()\n",
    "df[\"sku\"] = df[\"sku\"].astype(str).str.strip()\n",
    "df[\"category\"] = df[\"category\"].astype(str).str.strip()\n",
    "\n",
    "# --- Add B2B flag columns ---\n",
    "df[\"b2b_qty\"] = np.where(df[\"b2b\"].isna(), df[\"qty\"], 0)\n",
    "df[\"non_b2b_qty\"] = np.where(df[\"b2b\"].astype(str).str.lower() == \"false\", df[\"qty\"], 0)\n",
    "\n",
    "# --- Group by SKU, Category, MRP, Date ---\n",
    "grouped = (\n",
    "    df.groupby([\"sku\", \"category\", \"mrp\", \"date\"], as_index=False)\n",
    "      .agg(\n",
    "          qty_sold=(\"qty\", \"sum\"),\n",
    "          revenue=(\"amount\", \"sum\"),\n",
    "          orders=(\"order_id\", \"nunique\") if \"order_id\" in df.columns else (\"mrp\", \"count\"),\n",
    "          promo_qty=(\"qty\", lambda x: x[df.loc[x.index, \"promo_flag\"] == 1].sum()),\n",
    "          b2b_qty=(\"b2b_qty\", \"sum\"),\n",
    "          non_b2b_qty=(\"non_b2b_qty\", \"sum\")\n",
    "      )\n",
    ")\n",
    "\n",
    "# ============================\n",
    "# === Status breakdown\n",
    "# ============================\n",
    "\n",
    "# Canonical status map\n",
    "status_map = {\n",
    "    \"cancelled\": \"cancelled\",\n",
    "    \"canceled\": \"cancelled\",\n",
    "\n",
    "    \"shipped - delivered to buyer\": \"delivered_to_buyer\",\n",
    "    \"shipped\": \"intransit\",\n",
    "    \"shipped - returned to seller\": \"returned_to_seller\",\n",
    "    \"shipped - rejected by buyer\": \"rejected_by_buyer\",\n",
    "    \"shipped - picked up\": \"intransit\",\n",
    "    \"shipped - out for delivery\": \"intransit\",\n",
    "    \"shipped - lost in transit\": \"lost_in_transit\",\n",
    "    \"shipped - returning to seller\": \"returning_to_seller\",\n",
    "    \"shipped - damaged\": \"damaged\",\n",
    "    \"pending\": \"pending\",\n",
    "    \"pending - waiting for pick up\": \"waiting_for_pickup\",\n",
    "    \"shipping\": \"intransit\",\n",
    "}\n",
    "\n",
    "status_clean_raw = df[\"status\"].astype(str).str.strip().str.lower()\n",
    "df[\"status_clean\"] = status_clean_raw.map(status_map).fillna(\"other\")\n",
    "\n",
    "expected_status_cols = [\n",
    "    \"cancelled\",\n",
    "    \"delivered_to_buyer\",\n",
    "    \"intransit\",\n",
    "    \"returned_to_seller\",\n",
    "    \"rejected_by_buyer\",\n",
    "    \"lost_in_transit\",\n",
    "    \"returning_to_seller\",\n",
    "    \"pending\",\n",
    "    \"waiting_for_pickup\",\n",
    "    \"damaged\",\n",
    "    \"shipping\",\n",
    "    \"other\"\n",
    "]\n",
    "\n",
    "# Group and Pivot qty by status\n",
    "status_qty = (\n",
    "    df.groupby([\"sku\", \"category\", \"mrp\", \"date\", \"status_clean\"], as_index=False)[\"qty\"]\n",
    "      .sum()\n",
    "      .rename(columns={\"qty\": \"qty_sum\"})\n",
    ")\n",
    "\n",
    "pivot_status = (\n",
    "    status_qty\n",
    "    .pivot(index=[\"sku\", \"category\", \"mrp\", \"date\"], columns=\"status_clean\", values=\"qty_sum\")\n",
    "    .fillna(0)\n",
    ")\n",
    "\n",
    "# Ensure all expected columns exist\n",
    "for col in expected_status_cols:\n",
    "    if col not in pivot_status.columns:\n",
    "        pivot_status[col] = 0\n",
    "\n",
    "# Reorder + cast to int\n",
    "pivot_status = pivot_status[expected_status_cols].astype(int).reset_index()\n",
    "\n",
    "# ============================\n",
    "# === Final Merge\n",
    "# ============================\n",
    "\n",
    "final_product_data = grouped.merge(\n",
    "    pivot_status,\n",
    "    on=[\"sku\", \"category\", \"mrp\", \"date\"],\n",
    "    how=\"outer\"\n",
    ")\n",
    "\n",
    "# final_product_data is ready\n",
    "print(final_product_data.head())\n",
    "print(final_product_data.shape)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming `final_product_data` is already available\n",
    "df = final_product_data.copy()\n",
    "\n",
    "# --- Step 1: Compute shares ---\n",
    "df[\"promo_share\"] = df[\"promo_qty\"] / df[\"qty_sold\"]\n",
    "df[\"b2b_share\"] = df[\"b2b_qty\"] / df[\"qty_sold\"]\n",
    "df[\"non_b2b_share\"] = df[\"non_b2b_qty\"] / df[\"qty_sold\"]\n",
    "\n",
    "# --- Step 2: Assume COGS = 40% of MRP ---\n",
    "df[\"cogs\"] = df[\"mrp\"] * 0.40\n",
    "\n",
    "# --- Step 3: Define elasticity logic ---\n",
    "def suggest_price_adjustment(row):\n",
    "    promo = row[\"promo_share\"]\n",
    "    b2b = row[\"b2b_share\"]\n",
    "    cogs = row[\"cogs\"]\n",
    "    mrp = row[\"mrp\"]\n",
    "\n",
    "    # Case 1: Heavy promo + B2B reliance\n",
    "    if promo > 0.6 and b2b > 0.6:\n",
    "        return mrp * 0.85  # 15% reduction\n",
    "\n",
    "    # Case 2: Promo heavy but B2B low\n",
    "    elif promo > 0.6 and b2b <= 0.3:\n",
    "        return mrp * 0.90\n",
    "\n",
    "    # Case 3: B2B heavy but Promo low\n",
    "    elif b2b > 0.6 and promo <= 0.3:\n",
    "        return mrp * 0.90\n",
    "\n",
    "    # Case 4: Healthy mix\n",
    "    elif promo < 0.3 and b2b < 0.3:\n",
    "        return mrp * 1.05  # Suggest 5% increase\n",
    "\n",
    "    # Default: No change\n",
    "    return mrp\n",
    "\n",
    "df[\"suggested_mrp\"] = df.apply(suggest_price_adjustment, axis=1)\n",
    "\n",
    "# --- Output example ---\n",
    "output = df[[\"sku\", \"category\", \"date\", \"mrp\", \"qty_sold\", \"promo_share\", \"b2b_share\", \"suggested_mrp\"]]\n",
    "print(output.head())\n",
    "\n",
    "df.to_csv('conditional_mrp_for_sku.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8305e219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model performance on held-out set:\n",
      "       Model          MSE       R2\n",
      "RandomForest  1904.544120 0.974676\n",
      "       Lasso 57832.535643 0.231012\n",
      "       Ridge 57841.478870 0.230893\n",
      "      Linear 57841.517316 0.230892\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# === PREP FOR MACHINE LEARNING (FIXED) ===\n",
    "# ============================\n",
    "\n",
    "# Always use the aggregated table (has qty_sold, promo_qty, status cols, etc.)\n",
    "df_ml = final_product_data.copy()\n",
    "\n",
    "# --- Ensure required columns exist; if missing, create zeros ---\n",
    "for c in [\"promo_qty\", \"b2b_qty\", \"non_b2b_qty\", \"qty_sold\"]:\n",
    "    if c not in df_ml.columns:\n",
    "        df_ml[c] = 0\n",
    "\n",
    "# Some datasets may miss category; if so, synthesize a default\n",
    "if \"category\" not in df_ml.columns:\n",
    "    df_ml[\"category\"] = \"ALL\"\n",
    "\n",
    "# --- Encode categorical variables (handle NaN safely) ---\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "df_ml[\"sku\"] = df_ml[\"sku\"].astype(str).fillna(\"UNK\")\n",
    "df_ml[\"category\"] = df_ml[\"category\"].astype(str).fillna(\"UNK\")\n",
    "\n",
    "le_sku = LabelEncoder()\n",
    "le_cat = LabelEncoder()\n",
    "df_ml[\"sku_enc\"] = le_sku.fit_transform(df_ml[\"sku\"])\n",
    "df_ml[\"category_enc\"] = le_cat.fit_transform(df_ml[\"category\"])\n",
    "\n",
    "# --- Date features (robust parse then month/year) ---\n",
    "df_ml[\"date\"] = pd.to_datetime(df_ml[\"date\"], errors=\"coerce\")\n",
    "df_ml[\"month\"] = df_ml[\"date\"].dt.month.fillna(0).astype(int)\n",
    "df_ml[\"year\"]  = df_ml[\"date\"].dt.year.fillna(0).astype(int)\n",
    "\n",
    "# --- Safe division helper to prevent /0 and NaNs ---\n",
    "def safe_div(numer, denom):\n",
    "    return np.where(denom > 0, numer / denom, 0.0)\n",
    "\n",
    "# --- Share features ---\n",
    "df_ml[\"promo_share\"]     = safe_div(df_ml[\"promo_qty\"], df_ml[\"qty_sold\"])\n",
    "df_ml[\"b2b_share\"]       = safe_div(df_ml[\"b2b_qty\"], df_ml[\"qty_sold\"])\n",
    "df_ml[\"non_b2b_share\"]   = safe_div(df_ml[\"non_b2b_qty\"], df_ml[\"qty_sold\"])\n",
    "\n",
    "# --- Status percentage features (create if missing, then pct) ---\n",
    "status_cols = [\n",
    "    \"cancelled\", \"delivered_to_buyer\", \"intransit\", \"returned_to_seller\",\n",
    "    \"rejected_by_buyer\", \"lost_in_transit\", \"returning_to_seller\",\n",
    "    \"pending\", \"waiting_for_pickup\", \"damaged\", \"shipping\", \"other\"\n",
    "]\n",
    "for col in status_cols:\n",
    "    if col not in df_ml.columns:\n",
    "        df_ml[col] = 0\n",
    "    df_ml[f\"{col}_pct\"] = safe_div(df_ml[col], df_ml[\"qty_sold\"])\n",
    "\n",
    "# --- Final Features and Target ---\n",
    "base_features = [\"sku_enc\", \"category_enc\", \"month\", \"year\",\n",
    "                 \"promo_share\", \"b2b_share\", \"non_b2b_share\"]\n",
    "status_pct_feats = [f\"{col}_pct\" for col in status_cols]\n",
    "\n",
    "# Keep only columns that truly exist (defensive)\n",
    "features = [f for f in base_features + status_pct_feats if f in df_ml.columns]\n",
    "\n",
    "# Target is the price level (mrp) present in the aggregated table\n",
    "if \"mrp\" not in df_ml.columns:\n",
    "    raise ValueError(\"Target 'mrp' column not found in final_product_data. Check earlier grouping step.\")\n",
    "target = \"mrp\"\n",
    "\n",
    "X = df_ml[features].fillna(0)\n",
    "y = pd.to_numeric(df_ml[target], errors=\"coerce\")\n",
    "mask = y.notna()\n",
    "X, y = X[mask], y[mask]\n",
    "\n",
    "# --- Train/Test Split ---\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=42\n",
    ")\n",
    "\n",
    "# --- Try Multiple Regressors ---\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "models = {\n",
    "    \"Linear\": LinearRegression(),\n",
    "    \"Ridge\": Ridge(),\n",
    "    \"Lasso\": Lasso(),\n",
    "    \"RandomForest\": RandomForestRegressor(n_estimators=200, random_state=42)\n",
    "}\n",
    "\n",
    "rows = []\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, preds)\n",
    "    r2  = r2_score(y_test, preds) if np.isfinite(y_test).all() else np.nan\n",
    "    rows.append((name, mse, r2))\n",
    "\n",
    "results_df = pd.DataFrame(rows, columns=[\"Model\", \"MSE\", \"R2\"]).sort_values(\"MSE\")\n",
    "print(\"\\nModel performance on held-out set:\")\n",
    "print(results_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a31903",
   "metadata": {},
   "source": [
    "### checking the count of sku which have the sales consistently over 6 months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9b73c58f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SKUs with span > 6 months:\n",
      "                    min_date   max_date  months_span\n",
      "SKU                                                 \n",
      "BL006-54BLACK     2022-03-04 2022-12-05            9\n",
      "BL009-61BLACK     2022-01-04 2022-09-05            8\n",
      "BL009-61BLACK-B   2022-01-05 2022-10-06            9\n",
      "BL013-62BLACK     2022-03-05 2022-12-05            9\n",
      "BL035-161GOLD     2022-01-04 2022-12-05           11\n",
      "...                      ...        ...          ...\n",
      "SET433-KR-NP-XXXL 2022-03-06 2022-11-04            7\n",
      "SET435-KR-NP-L    2022-01-06 2022-09-06            8\n",
      "SET436-KR-NP-M    2022-01-06 2022-10-05            8\n",
      "SET436-KR-NP-XS   2022-04-06 2022-12-06            8\n",
      "SET436-KR-NP-XXXL 2022-01-05 2022-12-06           11\n",
      "\n",
      "[3224 rows x 3 columns]\n",
      "Count of SKUs: 3224\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load\n",
    "df = pd.read_csv(r\"C:/Users/MOHAMMED ARBAZ/Downloads/Amazon Sale Report.csv/Amazon Sale Report.csv\")\n",
    "\n",
    "# Parse dates robustly (handles 05-02-22 and 01-05-2022 etc.)\n",
    "df['date'] = pd.to_datetime(df['Date'].astype(str).str.strip(), dayfirst=True, errors='coerce')\n",
    "\n",
    "# Drop rows where date couldn't be parsed\n",
    "df = df.dropna(subset=['date'])\n",
    "\n",
    "# Min/Max per SKU\n",
    "rng = df.groupby('SKU')['date'].agg(min_date='min', max_date='max')\n",
    "\n",
    "# Month span (year*12 + month) and adjust if the end day is earlier than the start day\n",
    "month_span = (\n",
    "    (rng['max_date'].dt.year - rng['min_date'].dt.year) * 12\n",
    "    + (rng['max_date'].dt.month - rng['min_date'].dt.month)\n",
    "    - (rng['max_date'].dt.day < rng['min_date'].dt.day).astype(int)\n",
    ")\n",
    "\n",
    "# SKUs with span > 6 months\n",
    "result = rng.assign(months_span=month_span)\n",
    "result_over6 = result[result['months_span'] > 6]\n",
    "\n",
    "count_sku = result_over6.shape[0]\n",
    "\n",
    "print(\"SKUs with span > 6 months:\")\n",
    "print(result_over6)\n",
    "print(\"Count of SKUs:\", count_sku)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02d7ac0",
   "metadata": {},
   "source": [
    "# custom regressor model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c31032b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Step 0: Imports OK. Starting pipeline ===\n",
      "Reading CSV: C:/Users/MOHAMMED ARBAZ/Downloads/Amazon Sale Report.csv/Amazon Sale Report.csv\n",
      "Raw shape: (128975, 23)\n",
      "Columns: ['order_id', 'date', 'status', 'fulfilment', 'sales_channel', 'ship_service_level', 'style', 'sku', 'category', 'size', 'asin', 'courier_status', 'qty', 'currency', 'amount', 'ship_city', 'ship_state', 'ship_postal_code', 'ship_country', 'promotion_ids', 'b2b', 'fulfilled_by', 'unnamed:_22']\n",
      "Parsed dates. Bad/missing dates: 0\n",
      "Cleaned numerics & price. Dropped 15274 rows. Remaining: 113701\n",
      "Dropped cancelled rows: 5630\n",
      "Daily panel ready. Rows: 74390, Unique SKUs: 7093\n",
      "Starting per-SKU loop over 7093 SKUs...\n",
      "[1/7093] SKU=AN201-RED-M | Cat=Bottom | n_days=2 | latest=229.00\n",
      "  SARIMAX: not enough data or fit failed for AN201-RED-M\n",
      "  SARIMAX: not enough data or fit failed for AN201-RED-XL\n",
      "  SARIMAX: not enough data or fit failed for AN201-RED-XXL\n",
      "  SARIMAX: not enough data or fit failed for AN202-ORANGE-M\n",
      "  SARIMAX: not enough data or fit failed for AN202-ORANGE-S\n",
      "  SARIMAX: not enough data or fit failed for AN202-ORANGE-XXL\n",
      "  SARIMAX: not enough data or fit failed for AN203-MAROON-XXL\n",
      "  SARIMAX: not enough data or fit failed for AN204-PURPLE-L\n",
      "  SARIMAX: not enough data or fit failed for AN204-PURPLE-M\n",
      "[10/7093] SKU=AN204-PURPLE-S | Cat=Bottom | n_days=1 | latest=322.00\n",
      "  SARIMAX: not enough data or fit failed for AN204-PURPLE-S\n",
      "  SARIMAX: not enough data or fit failed for AN204-PURPLE-XXL\n",
      "  SARIMAX: not enough data or fit failed for AN205-YELLOW-S\n",
      "  SARIMAX: not enough data or fit failed for AN205-YELLOW-XL\n",
      "  SARIMAX: not enough data or fit failed for AN205-YELLOW-XXL\n",
      "  SARIMAX: not enough data or fit failed for AN206-GREEN-M\n",
      "  SARIMAX: not enough data or fit failed for AN206-GREEN-S\n",
      "  SARIMAX: not enough data or fit failed for AN206-GREEN-XL\n",
      "  SARIMAX: not enough data or fit failed for AN207-PINK-L\n",
      "  SARIMAX: not enough data or fit failed for AN207-PINK-S\n",
      "[20/7093] SKU=AN207-PINK-XXL | Cat=Bottom | n_days=1 | latest=322.00\n",
      "  SARIMAX: not enough data or fit failed for AN207-PINK-XXL\n",
      "  SARIMAX: not enough data or fit failed for AN208-MUSTARD-L\n",
      "  SARIMAX: not enough data or fit failed for AN208-MUSTARD-M\n",
      "  SARIMAX: not enough data or fit failed for AN208-MUSTARD-S\n",
      "  SARIMAX: not enough data or fit failed for AN208-MUSTARD-XL\n",
      "  SARIMAX: not enough data or fit failed for AN208-MUSTARD-XXL\n",
      "  SARIMAX: not enough data or fit failed for AN209-BIEGE-XL\n",
      "  SARIMAX: not enough data or fit failed for AN209-BIEGE-XXL\n",
      "  SARIMAX: not enough data or fit failed for AN211-BLACK-L\n",
      "  SARIMAX: not enough data or fit failed for AN211-BLACK-S\n",
      "[30/7093] SKU=AN211-BLACK-XL | Cat=Bottom | n_days=1 | latest=301.00\n",
      "  SARIMAX: not enough data or fit failed for AN211-BLACK-XL\n",
      "  SARIMAX: not enough data or fit failed for AN211-BLACK-XXL\n",
      "  SARIMAX: not enough data or fit failed for AN212-WHITE-L\n",
      "  SARIMAX: not enough data or fit failed for AN212-WHITE-M\n",
      "  SARIMAX: not enough data or fit failed for AN212-WHITE-S\n",
      "  SARIMAX: not enough data or fit failed for AN212-WHITE-XL\n",
      "  SARIMAX: not enough data or fit failed for AN212-WHITE-XXL\n",
      "  SARIMAX: not enough data or fit failed for AN213-BROWN-L\n",
      "  SARIMAX: not enough data or fit failed for AN213-BROWN-M\n",
      "  SARIMAX: not enough data or fit failed for AN213-BROWN-S\n",
      "[40/7093] SKU=AN213-BROWN-XL | Cat=Bottom | n_days=1 | latest=301.00\n",
      "  SARIMAX: not enough data or fit failed for AN213-BROWN-XL\n",
      "  SARIMAX: not enough data or fit failed for BL001-50PINK\n",
      "  SARIMAX: not enough data or fit failed for BL003-50BLACK\n",
      "  SARIMAX: not enough data or fit failed for BL003-50BLACK-B\n",
      "  SARIMAX: not enough data or fit failed for BL004-50CHIKU-B\n",
      "  SARIMAX: not enough data or fit failed for BL006-54BLACK\n",
      "  SARIMAX: not enough data or fit failed for BL006-54BLACK-B\n",
      "  SARIMAX: not enough data or fit failed for BL007-61PINK-B\n",
      "  SARIMAX: not enough data or fit failed for BL008-61RED-B\n",
      "  SARIMAX: not enough data or fit failed for BL009-61BLACK\n",
      "[50/7093] SKU=BL009-61BLACK-B | Cat=Blouse | n_days=14 | latest=494.00\n",
      "  SARIMAX: not enough data or fit failed for BL009-61BLACK-B\n",
      "  SARIMAX: not enough data or fit failed for BL010-61CHIKU-B\n",
      "  SARIMAX: not enough data or fit failed for BL011-62PINK-B\n",
      "  SARIMAX: not enough data or fit failed for BL013-62BLACK\n",
      "  SARIMAX: not enough data or fit failed for BL015-63PINK\n",
      "  SARIMAX: not enough data or fit failed for BL016-63RED-B\n",
      "  SARIMAX: not enough data or fit failed for BL017-63BLACK\n",
      "  SARIMAX: not enough data or fit failed for BL017-63BLACK-B\n",
      "  SARIMAX: not enough data or fit failed for BL019-71PINK\n",
      "  SARIMAX: not enough data or fit failed for BL019-71PINK-B\n",
      "[60/7093] SKU=BL020-71RED-B | Cat=Blouse | n_days=1 | latest=419.00\n",
      "  SARIMAX: not enough data or fit failed for BL020-71RED-B\n",
      "  SARIMAX: not enough data or fit failed for BL021-71BLACK\n",
      "  Fitted loglog on train (rows=13)\n",
      "  Fitted loglog_quad on train (rows=13)\n",
      "  Fit error fit_spline: Pandas data cast to numpy dtype of object. Check input data with np.asarray(data).\n",
      "  Fitted rf on train (rows=13)\n",
      "  Holdout loglog: {'r2_log': -10.1085, 'mae_qty': 1.1795}\n",
      "  Holdout loglog_quad: {'r2_log': -929.1135, 'mae_qty': 0.7354}\n",
      "  Holdout rf: {'r2_qty': -23.044, 'mae_qty': 0.9464}\n",
      "  SARIMAX: not enough data or fit failed for BL021-71BLACK-B\n",
      "  SARIMAX: not enough data or fit failed for BL022-71BEIGE\n",
      "  SARIMAX: not enough data or fit failed for BL023-74PINK-B\n",
      "  SARIMAX: not enough data or fit failed for BL024-74RED\n",
      "  SARIMAX: not enough data or fit failed for BL024-74RED-B\n",
      "  SARIMAX: not enough data or fit failed for BL025-74BLACK\n",
      "  SARIMAX: not enough data or fit failed for BL025-74BLACK-B\n",
      "  SARIMAX: not enough data or fit failed for BL026-74BEIGE\n",
      "[70/7093] SKU=BL026-74BEIGE-B | Cat=Blouse | n_days=2 | latest=383.00\n",
      "  SARIMAX: not enough data or fit failed for BL026-74BEIGE-B\n",
      "  SARIMAX: not enough data or fit failed for BL029-78BLACK\n",
      "  SARIMAX: not enough data or fit failed for BL029-78BLACK-B\n",
      "  SARIMAX: not enough data or fit failed for BL035-161GOLD\n",
      "  SARIMAX: not enough data or fit failed for BL036-176GOLD\n",
      "  SARIMAX: not enough data or fit failed for BL041-65RED-A\n",
      "  SARIMAX: not enough data or fit failed for BL050-83RED-A\n",
      "  SARIMAX: not enough data or fit failed for BL053-153GOLD-A\n",
      "  SARIMAX: not enough data or fit failed for BL055-164GOLD-A\n",
      "  SARIMAX: not enough data or fit failed for BL056-185GOLD\n",
      "[80/7093] SKU=BL057-65BLACK-A | Cat=Blouse | n_days=13 | latest=388.00\n",
      "  SARIMAX: not enough data or fit failed for BL057-65BLACK-A\n",
      "  SARIMAX: not enough data or fit failed for BL060-75RED\n",
      "  SARIMAX: not enough data or fit failed for BL073-85BLACK\n",
      "  SARIMAX: not enough data or fit failed for BL074-85RED\n",
      "  SARIMAX: not enough data or fit failed for BL075-85CHIKU\n",
      "  SARIMAX: not enough data or fit failed for BL079-87RED\n",
      "  SARIMAX: not enough data or fit failed for BL085-S\n",
      "  SARIMAX: not enough data or fit failed for BL086-XL\n",
      "  SARIMAX: not enough data or fit failed for BL086-XXL\n",
      "  SARIMAX: not enough data or fit failed for BL087-L\n",
      "[90/7093] SKU=BL087-M | Cat=Blouse | n_days=3 | latest=460.00\n",
      "  SARIMAX: not enough data or fit failed for BL087-M\n",
      "  SARIMAX: not enough data or fit failed for BL087-S\n",
      "  SARIMAX: not enough data or fit failed for BL087-XS\n",
      "  SARIMAX: not enough data or fit failed for BL087-XXL\n",
      "  SARIMAX: not enough data or fit failed for BL089-M\n",
      "  SARIMAX: not enough data or fit failed for BL089-XS\n",
      "  SARIMAX: not enough data or fit failed for BL089-XXL\n",
      "  SARIMAX: not enough data or fit failed for BL090-M\n",
      "  SARIMAX: not enough data or fit failed for BL090-S\n",
      "  SARIMAX: not enough data or fit failed for BL090-XXL\n",
      "[100/7093] SKU=BL093-M | Cat=Blouse | n_days=1 | latest=301.00\n",
      "  SARIMAX: not enough data or fit failed for BL093-M\n",
      "  SARIMAX: not enough data or fit failed for BL093-S\n",
      "  SARIMAX: not enough data or fit failed for BL095-M\n",
      "  SARIMAX: not enough data or fit failed for BL095-XL\n",
      "  SARIMAX: not enough data or fit failed for BL096-S\n",
      "  SARIMAX: not enough data or fit failed for BL096-XL\n",
      "  SARIMAX: not enough data or fit failed for BL098-M\n",
      "  SARIMAX: not enough data or fit failed for BL098-S\n",
      "  SARIMAX: not enough data or fit failed for BL098-XL\n",
      "  SARIMAX: not enough data or fit failed for BL098-XS\n",
      "[110/7093] SKU=BL098-XXL | Cat=Blouse | n_days=2 | latest=301.00\n",
      "  SARIMAX: not enough data or fit failed for BL098-XXL\n",
      "  SARIMAX: not enough data or fit failed for BL099-L\n",
      "  SARIMAX: not enough data or fit failed for BL099-S\n",
      "  SARIMAX: not enough data or fit failed for BL099-XS\n",
      "  SARIMAX: not enough data or fit failed for BL100-M\n",
      "  SARIMAX: not enough data or fit failed for BL100-S\n",
      "  SARIMAX: not enough data or fit failed for BL100-XL\n",
      "  SARIMAX: not enough data or fit failed for BL101-M\n",
      "  SARIMAX: not enough data or fit failed for BL101-S\n",
      "  SARIMAX: not enough data or fit failed for BL101-XS\n",
      "[120/7093] SKU=BL101-XXL | Cat=Blouse | n_days=1 | latest=249.00\n",
      "  SARIMAX: not enough data or fit failed for BL101-XXL\n",
      "  SARIMAX: not enough data or fit failed for BL102-L\n",
      "  SARIMAX: not enough data or fit failed for BL102-M\n",
      "  SARIMAX: not enough data or fit failed for BL102-S\n",
      "  SARIMAX: not enough data or fit failed for BL102-XS\n",
      "  SARIMAX: not enough data or fit failed for BL103-L\n",
      "  SARIMAX: not enough data or fit failed for BL103-M\n",
      "  SARIMAX: not enough data or fit failed for BL103-S\n",
      "  SARIMAX: not enough data or fit failed for BL103-XL\n",
      "  SARIMAX: not enough data or fit failed for BL103-XS\n",
      "[130/7093] SKU=BL103-XXL | Cat=Blouse | n_days=8 | latest=460.00\n",
      "  SARIMAX: not enough data or fit failed for BL103-XXL\n",
      "  SARIMAX: not enough data or fit failed for BL104-L\n",
      "  SARIMAX: not enough data or fit failed for BL104-M\n",
      "  SARIMAX: not enough data or fit failed for BL104-S\n",
      "  SARIMAX: not enough data or fit failed for BL104-XS\n",
      "  SARIMAX: not enough data or fit failed for BL104-XXL\n",
      "  SARIMAX: not enough data or fit failed for BL107-M\n",
      "  SARIMAX: not enough data or fit failed for BL107-S\n",
      "  SARIMAX: not enough data or fit failed for BL107-XL\n",
      "  SARIMAX: not enough data or fit failed for BL107-XS\n",
      "[140/7093] SKU=BL107-XXL | Cat=Blouse | n_days=1 | latest=311.00\n",
      "  SARIMAX: not enough data or fit failed for BL107-XXL\n",
      "  SARIMAX: not enough data or fit failed for BL109-L\n",
      "  SARIMAX: not enough data or fit failed for BL109-M\n",
      "  SARIMAX: not enough data or fit failed for BL109-S\n",
      "  SARIMAX: not enough data or fit failed for BL109-XL\n",
      "  SARIMAX: not enough data or fit failed for BL109-XS\n",
      "  SARIMAX: not enough data or fit failed for BL109-XXL\n",
      "  SARIMAX: not enough data or fit failed for BL110-L\n",
      "  SARIMAX: not enough data or fit failed for BL110-M\n",
      "  SARIMAX: not enough data or fit failed for BL110-S\n",
      "[150/7093] SKU=BL110-XL | Cat=Blouse | n_days=2 | latest=329.00\n",
      "  SARIMAX: not enough data or fit failed for BL110-XL\n",
      "  SARIMAX: not enough data or fit failed for BL110-XS\n",
      "  SARIMAX: not enough data or fit failed for BL110-XXL\n",
      "  SARIMAX: not enough data or fit failed for BL111-L\n",
      "  SARIMAX: not enough data or fit failed for BL111-M\n",
      "  SARIMAX: not enough data or fit failed for BL111-S\n",
      "  SARIMAX: not enough data or fit failed for BL111-XL\n",
      "  SARIMAX: not enough data or fit failed for BL111-XS\n",
      "  SARIMAX: not enough data or fit failed for BL113-L\n",
      "  SARIMAX: not enough data or fit failed for BL113-M\n",
      "[160/7093] SKU=BL113-S | Cat=Blouse | n_days=3 | latest=493.00\n",
      "  SARIMAX: not enough data or fit failed for BL113-S\n",
      "  SARIMAX: not enough data or fit failed for BL113-XL\n",
      "  SARIMAX: not enough data or fit failed for BL113-XS\n",
      "  SARIMAX: not enough data or fit failed for BL113-XXL\n",
      "  SARIMAX: not enough data or fit failed for BTM002-B-L\n",
      "  SARIMAX: not enough data or fit failed for BTM002-B-M\n",
      "  SARIMAX: not enough data or fit failed for BTM002-B-XL\n",
      "  SARIMAX: not enough data or fit failed for BTM002-B-XXXL\n",
      "  SARIMAX: not enough data or fit failed for BTM002-L\n",
      "  SARIMAX: not enough data or fit failed for BTM002-S\n",
      "[170/7093] SKU=BTM002-XL | Cat=Bottom | n_days=1 | latest=518.00\n",
      "  SARIMAX: not enough data or fit failed for BTM002-XL\n",
      "  SARIMAX: not enough data or fit failed for BTM002-XXXL\n",
      "  SARIMAX: not enough data or fit failed for BTM003-B-XXXL\n",
      "  SARIMAX: not enough data or fit failed for BTM003-M\n",
      "  SARIMAX: not enough data or fit failed for BTM003-XXXL\n",
      "  SARIMAX: not enough data or fit failed for BTM004-XXXL\n",
      "  SARIMAX: not enough data or fit failed for BTM005-B-L\n",
      "  SARIMAX: not enough data or fit failed for BTM005-L\n",
      "  SARIMAX: not enough data or fit failed for BTM005-XXL\n",
      "  SARIMAX: not enough data or fit failed for BTM005-XXXL\n",
      "[180/7093] SKU=BTM008-B-L | Cat=Bottom | n_days=2 | latest=518.00\n",
      "  SARIMAX: not enough data or fit failed for BTM008-B-L\n",
      "  SARIMAX: not enough data or fit failed for BTM008-B-M\n",
      "  SARIMAX: not enough data or fit failed for BTM008-B-XXL\n",
      "  SARIMAX: not enough data or fit failed for BTM008-L\n",
      "  SARIMAX: not enough data or fit failed for BTM008-XL\n",
      "  SARIMAX: not enough data or fit failed for BTM008-XXL\n",
      "  SARIMAX: not enough data or fit failed for BTM008-XXXL\n",
      "  SARIMAX: not enough data or fit failed for BTM021-B-L\n",
      "  SARIMAX: not enough data or fit failed for BTM021-B-XS\n",
      "  SARIMAX: not enough data or fit failed for BTM021-B-XXL\n",
      "[190/7093] SKU=BTM021-B-XXXL | Cat=Bottom | n_days=1 | latest=518.00\n",
      "  SARIMAX: not enough data or fit failed for BTM021-B-XXXL\n",
      "  SARIMAX: not enough data or fit failed for BTM026-NP-L\n",
      "  SARIMAX: not enough data or fit failed for BTM026-NP-M\n",
      "  SARIMAX: not enough data or fit failed for BTM026-NP-S\n",
      "  SARIMAX: not enough data or fit failed for BTM026-NP-XL\n",
      "  SARIMAX: not enough data or fit failed for BTM026-NP-XS\n",
      "  SARIMAX: not enough data or fit failed for BTM026-NP-XXL\n",
      "  SARIMAX: not enough data or fit failed for BTM026-NP-XXXL\n",
      "  SARIMAX: not enough data or fit failed for BTM027-NP-XXL\n",
      "  SARIMAX: not enough data or fit failed for BTM027-NP-XXXL\n",
      "[200/7093] SKU=BTM029-NP-L | Cat=Bottom | n_days=2 | latest=518.00\n",
      "  SARIMAX: not enough data or fit failed for BTM029-NP-L\n",
      "  SARIMAX: not enough data or fit failed for BTM029-NP-M\n",
      "  SARIMAX: not enough data or fit failed for BTM029-NP-S\n",
      "  SARIMAX: not enough data or fit failed for BTM029-NP-XL\n",
      "  SARIMAX: not enough data or fit failed for BTM029-NP-XS\n",
      "  SARIMAX: not enough data or fit failed for BTM029-NP-XXL\n",
      "  SARIMAX: not enough data or fit failed for BTM029-NP-XXXL\n",
      "  SARIMAX: not enough data or fit failed for BTM030-NP-M\n",
      "  SARIMAX: not enough data or fit failed for BTM030-NP-XL\n",
      "  SARIMAX: not enough data or fit failed for BTM030-NP-XS\n",
      "[210/7093] SKU=BTM030-NP-XXL | Cat=Bottom | n_days=1 | latest=284.00\n",
      "  SARIMAX: not enough data or fit failed for BTM030-NP-XXL\n",
      "  SARIMAX: not enough data or fit failed for BTM030-NP-XXXL\n",
      "  SARIMAX: not enough data or fit failed for BTM031-NP-L\n",
      "  SARIMAX: not enough data or fit failed for BTM031-NP-M\n",
      "  SARIMAX: not enough data or fit failed for BTM031-NP-XL\n",
      "  SARIMAX: not enough data or fit failed for BTM031-NP-XS\n",
      "  SARIMAX: not enough data or fit failed for BTM031-NP-XXL\n",
      "  SARIMAX: not enough data or fit failed for BTM032-NP-S\n",
      "  SARIMAX: not enough data or fit failed for BTM032-NP-XS\n",
      "  SARIMAX: not enough data or fit failed for BTM033-NP-XS\n",
      "[220/7093] SKU=BTM033-NP-XXL | Cat=Bottom | n_days=1 | latest=297.00\n",
      "  SARIMAX: not enough data or fit failed for BTM033-NP-XXL\n",
      "  SARIMAX: not enough data or fit failed for BTM033-NP-XXXL\n",
      "  SARIMAX: not enough data or fit failed for BTM035-NP-XS\n",
      "  SARIMAX: not enough data or fit failed for BTM035-NP-XXXL\n",
      "  SARIMAX: not enough data or fit failed for BTM036-PP-L\n",
      "  SARIMAX: not enough data or fit failed for BTM036-PP-M\n",
      "  SARIMAX: not enough data or fit failed for BTM036-PP-S\n",
      "  SARIMAX: not enough data or fit failed for BTM036-PP-XL\n",
      "  SARIMAX: not enough data or fit failed for BTM036-PP-XXL\n",
      "  SARIMAX: not enough data or fit failed for BTM036-PP-XXXL\n",
      "[230/7093] SKU=BTM037-PP-M | Cat=Bottom | n_days=1 | latest=345.00\n",
      "  SARIMAX: not enough data or fit failed for BTM037-PP-M\n",
      "  SARIMAX: not enough data or fit failed for BTM037-PP-S\n",
      "  SARIMAX: not enough data or fit failed for BTM037-PP-XL\n",
      "  SARIMAX: not enough data or fit failed for BTM037-PP-XS\n",
      "  SARIMAX: not enough data or fit failed for BTM037-PP-XXL\n",
      "  SARIMAX: not enough data or fit failed for BTM037-PP-XXXL\n",
      "  SARIMAX: not enough data or fit failed for BTM038-PP-L\n",
      "  SARIMAX: not enough data or fit failed for BTM038-PP-S\n",
      "  SARIMAX: not enough data or fit failed for BTM038-PP-XS\n",
      "  SARIMAX: not enough data or fit failed for BTM038-PP-XXL\n",
      "[240/7093] SKU=BTM038-PP-XXXL | Cat=Bottom | n_days=6 | latest=379.00\n",
      "  SARIMAX: not enough data or fit failed for BTM038-PP-XXXL\n",
      "  SARIMAX: not enough data or fit failed for BTM039-PP-L\n",
      "  SARIMAX: not enough data or fit failed for BTM039-PP-M\n",
      "  SARIMAX: not enough data or fit failed for BTM039-PP-S\n",
      "  SARIMAX: not enough data or fit failed for BTM039-PP-XL\n",
      "  SARIMAX: not enough data or fit failed for BTM039-PP-XS\n",
      "  SARIMAX: not enough data or fit failed for BTM039-PP-XXL\n",
      "  SARIMAX: not enough data or fit failed for BTM039-PP-XXXL\n",
      "  SARIMAX: not enough data or fit failed for BTM040-PP-M\n",
      "  SARIMAX: not enough data or fit failed for BTM040-PP-S\n",
      "[250/7093] SKU=BTM040-PP-XL | Cat=Bottom | n_days=2 | latest=356.00\n",
      "  SARIMAX: not enough data or fit failed for BTM040-PP-XL\n",
      "  SARIMAX: not enough data or fit failed for BTM040-PP-XS\n",
      "  SARIMAX: not enough data or fit failed for BTM040-PP-XXL\n",
      "  SARIMAX: not enough data or fit failed for BTM040-PP-XXXL\n",
      "  SARIMAX: not enough data or fit failed for BTM041-PP-XL\n",
      "  SARIMAX: not enough data or fit failed for BTM041-PP-XS\n",
      "  SARIMAX: not enough data or fit failed for BTM041-PP-XXL\n",
      "  SARIMAX: not enough data or fit failed for BTM041-PP-XXXL\n",
      "  SARIMAX: not enough data or fit failed for BTM042-PP-L\n",
      "  SARIMAX: not enough data or fit failed for BTM042-PP-M\n",
      "[260/7093] SKU=BTM042-PP-S | Cat=Bottom | n_days=1 | latest=317.00\n",
      "  SARIMAX: not enough data or fit failed for BTM042-PP-S\n",
      "  SARIMAX: not enough data or fit failed for BTM042-PP-XL\n",
      "  SARIMAX: not enough data or fit failed for BTM042-PP-XS\n",
      "  SARIMAX: not enough data or fit failed for BTM042-PP-XXL\n",
      "  SARIMAX: not enough data or fit failed for BTM042-PP-XXXL\n",
      "  SARIMAX: not enough data or fit failed for BTM043-PP-S\n",
      "  SARIMAX: not enough data or fit failed for BTM043-PP-XL\n",
      "  SARIMAX: not enough data or fit failed for BTM043-PP-XS\n",
      "  SARIMAX: not enough data or fit failed for BTM043-PP-XXL\n",
      "  SARIMAX: not enough data or fit failed for BTM043-PP-XXXL\n",
      "[270/7093] SKU=BTM044-PP-M | Cat=Bottom | n_days=3 | latest=360.00\n",
      "  SARIMAX: not enough data or fit failed for BTM044-PP-M\n",
      "  SARIMAX: not enough data or fit failed for BTM044-PP-XL\n",
      "  SARIMAX: not enough data or fit failed for BTM044-PP-XS\n",
      "  SARIMAX: not enough data or fit failed for BTM044-PP-XXL\n",
      "  SARIMAX: not enough data or fit failed for BTM044-PP-XXXL\n",
      "  SARIMAX: not enough data or fit failed for BTM045-PP-M\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 402\u001b[0m\n\u001b[0;32m    400\u001b[0m sarimax_res, sarimax_val \u001b[38;5;241m=\u001b[39m sarimax_fit_and_validate(g)\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sarimax_res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 402\u001b[0m     \u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m  SARIMAX: not enough data or fit failed for \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msku\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    404\u001b[0m \u001b[38;5;66;03m# Skip if too little data for price models\u001b[39;00m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_days \u001b[38;5;241m<\u001b[39m MIN_DAILY_ROWS_SKU:\n",
      "Cell \u001b[1;32mIn[27], line 61\u001b[0m, in \u001b[0;36mlog\u001b[1;34m(msg)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlog\u001b[39m(msg):\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m VERBOSE:\n\u001b[1;32m---> 61\u001b[0m         \u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflush\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\ipykernel\\iostream.py:609\u001b[0m, in \u001b[0;36mOutStream.flush\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    607\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpub_thread\u001b[38;5;241m.\u001b[39mschedule(evt\u001b[38;5;241m.\u001b[39mset)\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;66;03m# and give a timeout to avoid\u001b[39;00m\n\u001b[1;32m--> 609\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mevt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflush_timeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    610\u001b[0m         \u001b[38;5;66;03m# write directly to __stderr__ instead of warning because\u001b[39;00m\n\u001b[0;32m    611\u001b[0m         \u001b[38;5;66;03m# if this is happening sys.stderr may be the problem.\u001b[39;00m\n\u001b[0;32m    612\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIOStream.flush timed out\u001b[39m\u001b[38;5;124m\"\u001b[39m, file\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39m__stderr__)\n\u001b[0;32m    613\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\MOHAMMED ARBAZ\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\threading.py:659\u001b[0m, in \u001b[0;36mEvent.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    657\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[0;32m    658\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[1;32m--> 659\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    660\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[1;32mc:\\Users\\MOHAMMED ARBAZ\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\threading.py:363\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 363\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    364\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    365\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ===========================================\n",
    "# PRICE OPTIMIZATION + SARIMAX FORECAST (PER SKU) with DEBUG + HOLDOUT\n",
    "# ===========================================\n",
    "\n",
    "import warnings, time, os\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "# SARIMAX\n",
    "try:\n",
    "    from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "    HAS_SARIMAX = True\n",
    "except Exception:\n",
    "    HAS_SARIMAX = False\n",
    "\n",
    "# Spline (optional)\n",
    "try:\n",
    "    import statsmodels.api as sm\n",
    "    from patsy import dmatrix\n",
    "    HAS_PATSY = True\n",
    "except Exception:\n",
    "    HAS_PATSY = False\n",
    "\n",
    "# -------------------------------\n",
    "# CONFIG\n",
    "# -------------------------------\n",
    "CSV_PATH = r\"C:/Users/MOHAMMED ARBAZ/Downloads/Amazon Sale Report.csv/Amazon Sale Report.csv\"\n",
    "\n",
    "# Costs\n",
    "COST_COLUMN = None\n",
    "COST_FRACTION_OF_PRICE = 0.70\n",
    "\n",
    "# Price grid / compute\n",
    "PRICE_BOUND_PCT = 0.25\n",
    "N_PRICE_GRID = 25\n",
    "FORECAST_HORIZON = 30\n",
    "\n",
    "# SARIMAX params\n",
    "SARIMAX_ORDER = (1, 1, 1)\n",
    "SARIMAX_SEASONAL_ORDER = (1, 1, 1, 7)\n",
    "SARIMAX_VAL_DAYS_MIN = 14   # min days for SARIMAX validation\n",
    "\n",
    "# Data thresholds\n",
    "MIN_DAILY_ROWS_SKU = 14\n",
    "MIN_DAILY_ROWS_SARIMA = 30\n",
    "\n",
    "# Holdout split for regressors\n",
    "HOLDOUT_FRACTION = 0.20     # last 20% of days as test; min 7 days enforced\n",
    "\n",
    "# Debug / verbosity\n",
    "VERBOSE = True\n",
    "PRINT_EVERY_SKU = 10        # print progress every N SKUs\n",
    "\n",
    "def log(msg):\n",
    "    if VERBOSE:\n",
    "        print(msg, flush=True)\n",
    "\n",
    "t0 = time.time()\n",
    "log(\"=== Step 0: Imports OK. Starting pipeline ===\")\n",
    "\n",
    "# -------------------------------\n",
    "# 1) LOAD & CLEAN\n",
    "# -------------------------------\n",
    "def normalize_col(c):\n",
    "    return (\n",
    "        str(c).strip()\n",
    "        .replace(\"\\xa0\", \" \")\n",
    "        .replace(\"\\u200b\", \"\")\n",
    "        .strip()\n",
    "        .lower()\n",
    "        .replace(\" \", \"_\")\n",
    "        .replace(\"-\", \"_\")\n",
    "    )\n",
    "\n",
    "log(f\"Reading CSV: {CSV_PATH}\")\n",
    "df_raw = pd.read_csv(CSV_PATH)\n",
    "df_raw.columns = [normalize_col(c) for c in df_raw.columns]\n",
    "df_raw = df_raw.drop(columns=['index'])\n",
    "log(f\"Raw shape: {df_raw.shape}\")\n",
    "log(f\"Columns: {list(df_raw.columns)}\")\n",
    "\n",
    "date_col = \"date\" if \"date\" in df_raw.columns else (\"order_date\" if \"order_date\" in df_raw.columns else None)\n",
    "if date_col is None:\n",
    "    raise ValueError(\"No date column found ('date' or 'order_date').\")\n",
    "\n",
    "sku_col = \"sku\"\n",
    "qty_col = \"qty\" if \"qty\" in df_raw.columns else (\"quantity\" if \"quantity\" in df_raw.columns else None)\n",
    "amount_col = \"amount\" if \"amount\" in df_raw.columns else (\"revenue\" if \"revenue\" in df_raw.columns else None)\n",
    "cat_col = \"category\" if \"category\" in df_raw.columns else None\n",
    "if not all([sku_col in df_raw.columns, qty_col, amount_col]):\n",
    "    raise ValueError(\"Missing required columns: sku, qty/quantity, amount/revenue.\")\n",
    "\n",
    "# Parse dates (month-first pref, fallback to day-first)\n",
    "d_pref = pd.to_datetime(df_raw[date_col].astype(str).str.strip(), errors=\"coerce\", dayfirst=False)\n",
    "d_fall = pd.to_datetime(df_raw[date_col].astype(str).str.strip(), errors=\"coerce\", dayfirst=True)\n",
    "df = df_raw.copy()\n",
    "df[\"date_parsed\"] = d_pref.fillna(d_fall)\n",
    "df[\"day\"] = pd.to_datetime(df[\"date_parsed\"].dt.date)\n",
    "n_bad_dates = df[\"day\"].isna().sum()\n",
    "log(f\"Parsed dates. Bad/missing dates: {n_bad_dates}\")\n",
    "\n",
    "# numerics & price\n",
    "df[qty_col] = pd.to_numeric(df[qty_col], errors=\"coerce\")\n",
    "df[amount_col] = pd.to_numeric(df[amount_col], errors=\"coerce\")\n",
    "before = df.shape[0]\n",
    "df = df.dropna(subset=[\"day\", qty_col, amount_col])\n",
    "df = df[(df[qty_col] > 0) & (df[amount_col] > 0)].copy()\n",
    "df[\"price\"] = df[amount_col] / df[qty_col]\n",
    "df = df[np.isfinite(df[\"price\"]) & (df[\"price\"] > 0)].copy()\n",
    "log(f\"Cleaned numerics & price. Dropped {before - df.shape[0]} rows. Remaining: {df.shape[0]}\")\n",
    "\n",
    "# promo flag\n",
    "promo_col = \"promotion_ids\" if \"promotion_ids\" in df.columns else None\n",
    "df[\"promo_flag\"] = df[promo_col].notna().astype(int) if promo_col else 0\n",
    "\n",
    "# drop canceled if present\n",
    "if \"status\" in df.columns:\n",
    "    s = df[\"status\"].astype(str).str.lower().str.strip()\n",
    "    before = df.shape[0]\n",
    "    df = df[~s.isin([\"cancelled\", \"canceled\"])]\n",
    "    log(f\"Dropped cancelled rows: {before - df.shape[0]}\")\n",
    "\n",
    "# categories fallback\n",
    "if cat_col is None:\n",
    "    df[\"category\"] = \"ALL\"\n",
    "    cat_col = \"category\"\n",
    "\n",
    "df[sku_col] = df[sku_col].astype(str).str.strip()\n",
    "df[cat_col] = df[cat_col].astype(str).str.strip()\n",
    "\n",
    "# -------------------------------\n",
    "# 2) DAILY AGG PER SKU\n",
    "# -------------------------------\n",
    "daily = (\n",
    "    df.groupby([sku_col, cat_col, \"day\"], as_index=False)\n",
    "      .agg(qty=(qty_col, \"sum\"),\n",
    "           revenue=(amount_col, \"sum\"),\n",
    "           price=(\"price\", \"median\"),\n",
    "           promo_flag=(\"promo_flag\", \"max\"))\n",
    ")\n",
    "daily[\"dow\"] = daily[\"day\"].dt.dayofweek\n",
    "daily[\"month\"] = daily[\"day\"].dt.month\n",
    "daily[\"ln_q\"] = np.log(daily[\"qty\"] + 1e-6)\n",
    "daily[\"ln_p\"] = np.log(daily[\"price\"])\n",
    "\n",
    "dow_d = pd.get_dummies(daily[\"dow\"], prefix=\"dow\", drop_first=True)\n",
    "mon_d = pd.get_dummies(daily[\"month\"], prefix=\"m\", drop_first=True)\n",
    "daily = pd.concat([daily, dow_d, mon_d], axis=1)\n",
    "CTRL_COLS = [c for c in daily.columns if c.startswith(\"dow_\") or c.startswith(\"m_\")] + [\"promo_flag\"]\n",
    "\n",
    "n_skus = daily[[sku_col, cat_col]].drop_duplicates().shape[0]\n",
    "log(f\"Daily panel ready. Rows: {daily.shape[0]}, Unique SKUs: {n_skus}\")\n",
    "\n",
    "# SKU price stats\n",
    "sku_price_stats = (\n",
    "    daily.sort_values(\"day\")\n",
    "         .groupby([sku_col, cat_col])\n",
    "         .agg(latest_price=(\"price\", \"last\"),\n",
    "              min_price=(\"price\", \"min\"),\n",
    "              max_price=(\"price\", \"max\"),\n",
    "              n_days=(\"day\", \"nunique\"))\n",
    "         .reset_index()\n",
    ")\n",
    "\n",
    "# costs (prefer actual column)\n",
    "if COST_COLUMN and COST_COLUMN in df.columns:\n",
    "    last_cost = (\n",
    "        df.sort_values(\"day\")\n",
    "          .groupby([sku_col, cat_col])\n",
    "          .tail(1)[[sku_col, cat_col, COST_COLUMN]]\n",
    "          .rename(columns={COST_COLUMN: \"unit_cost\"})\n",
    "    )\n",
    "    sku_price_stats = sku_price_stats.merge(last_cost, on=[sku_col, cat_col], how=\"left\")\n",
    "else:\n",
    "    sku_price_stats[\"unit_cost\"] = np.nan  # fill later from fraction\n",
    "\n",
    "def make_price_grid(latest, lo_obs, hi_obs, pct=PRICE_BOUND_PCT, n=N_PRICE_GRID):\n",
    "    lo = max(0.01, latest * (1 - pct), lo_obs)\n",
    "    hi = max(lo + 1e-6, latest * (1 + pct), hi_obs)\n",
    "    return np.linspace(lo, hi, n)\n",
    "\n",
    "# -------------------------------\n",
    "# 3) PRICE->QTY MODELS + HOLDOUT\n",
    "# -------------------------------\n",
    "def time_split(g):\n",
    "    \"\"\"Return g_train, g_test (time-based split).\"\"\"\n",
    "    g = g.sort_values(\"day\")\n",
    "    unique_days = g[\"day\"].drop_duplicates().tolist()\n",
    "    n = len(unique_days)\n",
    "    n_test = max(7, int(np.ceil(n * HOLDOUT_FRACTION)))\n",
    "    if n <= n_test + 5:  # ensure train not too tiny\n",
    "        return g, None\n",
    "    cutoff = unique_days[-n_test]\n",
    "    g_train = g[g[\"day\"] < cutoff].copy()\n",
    "    g_test = g[g[\"day\"] >= cutoff].copy()\n",
    "    return g_train, g_test\n",
    "\n",
    "def fit_loglog(g):\n",
    "    X = g[[\"ln_p\"] + CTRL_COLS]\n",
    "    y = g[\"ln_q\"]\n",
    "    if len(g) < 8:\n",
    "        return None\n",
    "    lr = LinearRegression().fit(X, y)\n",
    "    return (\"loglog\", lr, X.columns.tolist())\n",
    "\n",
    "def fit_loglog_quad(g):\n",
    "    X = g[[\"ln_p\"]].copy()\n",
    "    X[\"ln_p2\"] = X[\"ln_p\"]**2\n",
    "    X = pd.concat([X, g[CTRL_COLS]], axis=1)\n",
    "    y = g[\"ln_q\"]\n",
    "    if len(g) < 10:\n",
    "        return None\n",
    "    lr = LinearRegression().fit(X, y)\n",
    "    return (\"loglog_quad\", lr, X.columns.tolist())\n",
    "\n",
    "def fit_spline(g):\n",
    "    if not HAS_PATSY:\n",
    "        return None\n",
    "    design = dmatrix(\"bs(ln_p, df=4, include_intercept=True)\", data=g, return_type=\"dataframe\")\n",
    "    X = pd.concat([design, g[CTRL_COLS]], axis=1)\n",
    "    y = g[\"ln_q\"]\n",
    "    if len(g) < 12:\n",
    "        return None\n",
    "    model = sm.OLS(y, sm.add_constant(X, has_constant=\"add\")).fit()\n",
    "    return (\"spline\", model, X.columns.tolist(), True)\n",
    "\n",
    "def fit_rf(g):\n",
    "    X = g[[\"price\"] + CTRL_COLS]\n",
    "    y = g[\"qty\"]\n",
    "    if len(g) < 12:\n",
    "        return None\n",
    "    rf = RandomForestRegressor(n_estimators=300, random_state=42)\n",
    "    rf.fit(X, y)\n",
    "    return (\"rf\", rf, X.columns.tolist())\n",
    "\n",
    "def predict_qty_from_model(model_tuple, dfX):\n",
    "    name = model_tuple[0]\n",
    "    if name in (\"loglog\", \"loglog_quad\"):\n",
    "        lr, cols = model_tuple[1], model_tuple[2]\n",
    "        dfX = dfX.reindex(columns=cols, fill_value=0.0)\n",
    "        ln_q = lr.predict(dfX.values)\n",
    "        return np.exp(ln_q).clip(min=0.0)\n",
    "    if name == \"spline\":\n",
    "        model, cols, _ = model_tuple[1], model_tuple[2], model_tuple[3]\n",
    "        dfX = sm.add_constant(dfX.reindex(columns=cols, fill_value=0.0), has_constant=\"add\")\n",
    "        ln_q = model.predict(dfX)\n",
    "        return np.exp(ln_q).clip(min=0.0)\n",
    "    if name == \"rf\":\n",
    "        rf, cols = model_tuple[1], model_tuple[2]\n",
    "        dfX = dfX.reindex(columns=cols, fill_value=0.0)\n",
    "        q = rf.predict(dfX.values)\n",
    "        return np.maximum(q, 0.0)\n",
    "    raise ValueError(\"Unknown model\")\n",
    "\n",
    "def build_grid_frame(price_grid, avg_ctrls, model_name):\n",
    "    if model_name in (\"loglog\", \"loglog_quad\", \"spline\"):\n",
    "        dfX = pd.DataFrame({\"ln_p\": np.log(price_grid)})\n",
    "        if model_name == \"loglog_quad\":\n",
    "            dfX[\"ln_p2\"] = dfX[\"ln_p\"]**2\n",
    "        for c in CTRL_COLS:\n",
    "            dfX[c] = avg_ctrls.get(c, 0.0)\n",
    "        return dfX\n",
    "    else:\n",
    "        dfX = pd.DataFrame({\"price\": price_grid})\n",
    "        for c in CTRL_COLS:\n",
    "            dfX[c] = avg_ctrls.get(c, 0.0)\n",
    "        return dfX\n",
    "\n",
    "def eval_holdout(model_tuple, g_train, g_test):\n",
    "    \"\"\"Return dict of holdout metrics for model_tuple.\"\"\"\n",
    "    if g_test is None or len(g_test) == 0:\n",
    "        return {}\n",
    "    name = model_tuple[0]\n",
    "    if name in (\"loglog\", \"loglog_quad\", \"spline\"):\n",
    "        if name == \"loglog\":\n",
    "            Xtest = g_test[[\"ln_p\"] + CTRL_COLS]\n",
    "        elif name == \"loglog_quad\":\n",
    "            Xtest = g_test[[\"ln_p\"]].copy()\n",
    "            Xtest[\"ln_p2\"] = Xtest[\"ln_p\"]**2\n",
    "            Xtest = pd.concat([Xtest, g_test[CTRL_COLS]], axis=1)\n",
    "        else:  # spline\n",
    "            design = dmatrix(\"bs(ln_p, df=4, include_intercept=True)\", data=g_test, return_type=\"dataframe\")\n",
    "            Xtest = pd.concat([design, g_test[CTRL_COLS]], axis=1)\n",
    "        y_true_ln = g_test[\"ln_q\"].values\n",
    "        q_pred = predict_qty_from_model(model_tuple, Xtest)\n",
    "        y_pred_ln = np.log(q_pred + 1e-6)\n",
    "        r2_log = r2_score(y_true_ln, y_pred_ln) if np.isfinite(y_true_ln).all() else np.nan\n",
    "        mae_qty = mean_absolute_error(g_test[\"qty\"].values, q_pred)\n",
    "        return {\"r2_log\": round(float(r2_log), 4) if np.isfinite(r2_log) else np.nan,\n",
    "                \"mae_qty\": round(float(mae_qty), 4)}\n",
    "    else:\n",
    "        Xtest = g_test[[\"price\"] + CTRL_COLS]\n",
    "        q_true = g_test[\"qty\"].values\n",
    "        q_pred = predict_qty_from_model(model_tuple, Xtest)\n",
    "        r2_qty = r2_score(q_true, q_pred) if len(np.unique(q_true)) > 1 else np.nan\n",
    "        mae_qty = mean_absolute_error(q_true, q_pred)\n",
    "        return {\"r2_qty\": round(float(r2_qty), 4) if np.isfinite(r2_qty) else np.nan,\n",
    "                \"mae_qty\": round(float(mae_qty), 4)}\n",
    "\n",
    "# -------------------------------\n",
    "# 4) SARIMAX helpers (with validation)\n",
    "# -------------------------------\n",
    "def sarimax_fit_and_validate(g):\n",
    "    \"\"\"Return (res, val_metrics_dict). Fit on train, validate on last SARIMAX_VAL_DAYS_MIN days.\"\"\"\n",
    "    if not HAS_SARIMAX or len(g) < max(MIN_DAILY_ROWS_SARIMA, SARIMAX_VAL_DAYS_MIN+10):\n",
    "        return None, {}\n",
    "    g = g.sort_values(\"day\")\n",
    "    unique_days = g[\"day\"].drop_duplicates().tolist()\n",
    "    if len(unique_days) < SARIMAX_VAL_DAYS_MIN + 10:\n",
    "        return None, {}\n",
    "\n",
    "    cutoff = unique_days[-SARIMAX_VAL_DAYS_MIN]\n",
    "    g_train = g[g[\"day\"] < cutoff]\n",
    "    g_test = g[g[\"day\"] >= cutoff]\n",
    "    if len(g_train) < MIN_DAILY_ROWS_SARIMA:\n",
    "        return None, {}\n",
    "\n",
    "    s_tr = g_train.set_index(\"day\").asfreq(\"D\")\n",
    "    for c in [\"qty\", \"price\", \"promo_flag\"]:\n",
    "        if c not in s_tr.columns: s_tr[c] = 0\n",
    "    s_tr[\"qty\"] = s_tr[\"qty\"].fillna(0)\n",
    "    s_tr[\"price\"] = s_tr[\"price\"].ffill()\n",
    "    s_tr[\"promo_flag\"] = s_tr[\"promo_flag\"].fillna(0)\n",
    "    y_tr = s_tr[\"qty\"]\n",
    "    ex_tr = s_tr[[\"price\", \"promo_flag\"]]\n",
    "\n",
    "    try:\n",
    "        mod = SARIMAX(y_tr, order=SARIMAX_ORDER, seasonal_order=SARIMAX_SEASONAL_ORDER,\n",
    "                      exog=ex_tr, enforce_stationarity=False, enforce_invertibility=False)\n",
    "        res = mod.fit(disp=False)\n",
    "    except Exception:\n",
    "        return None, {}\n",
    "\n",
    "    # validate on test using actual future exog\n",
    "    s_te = g_test.set_index(\"day\").asfreq(\"D\")\n",
    "    s_te[\"qty\"] = s_te[\"qty\"].fillna(0)\n",
    "    s_te[\"price\"] = s_te[\"price\"].ffill()\n",
    "    s_te[\"promo_flag\"] = s_te[\"promo_flag\"].fillna(0)\n",
    "    ex_te = s_te[[\"price\", \"promo_flag\"]]\n",
    "    try:\n",
    "        fc = res.get_forecast(steps=len(s_te), exog=ex_te)\n",
    "        pred = np.maximum(fc.predicted_mean.values, 0.0)\n",
    "        true = s_te[\"qty\"].values\n",
    "        rmse = mean_squared_error(true, pred, squared=False)\n",
    "        mape = np.mean(np.abs((true - pred) / np.maximum(true, 1e-6))) * 100.0\n",
    "        return res, {\"sarimax_rmse\": round(float(rmse), 4), \"sarimax_mape\": round(float(mape), 2)}\n",
    "    except Exception:\n",
    "        return res, {}\n",
    "\n",
    "def sarimax_forecast_for_price_with_model(res, g, future_price, horizon=FORECAST_HORIZON):\n",
    "    \"\"\"Use a fitted SARIMAX result (res) to forecast next horizon with constant future price.\"\"\"\n",
    "    s = g.set_index(\"day\").asfreq(\"D\")\n",
    "    s[\"qty\"] = s[\"qty\"].fillna(0)\n",
    "    s[\"price\"] = s[\"price\"].ffill()\n",
    "    s[\"promo_flag\"] = s[\"promo_flag\"].fillna(0)\n",
    "    y = s[\"qty\"]\n",
    "    # refresh state with full history\n",
    "    try:\n",
    "        res_full = res.model.smooth(res.params)\n",
    "    except Exception:\n",
    "        res_full = res\n",
    "    future_idx = pd.date_range(y.index[-1] + pd.Timedelta(days=1), periods=horizon, freq=\"D\")\n",
    "    future_exog = pd.DataFrame({\"price\": [future_price]*horizon, \"promo_flag\": [0]*horizon}, index=future_idx)\n",
    "    try:\n",
    "        fc = res_full.get_forecast(steps=horizon, exog=future_exog)\n",
    "        return np.maximum(fc.predicted_mean.values, 0.0)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# -------------------------------\n",
    "# 5) PER-SKU LOOP\n",
    "# -------------------------------\n",
    "records = []\n",
    "skus = daily[[sku_col, cat_col]].drop_duplicates().values.tolist()\n",
    "log(f\"Starting per-SKU loop over {len(skus)} SKUs...\")\n",
    "\n",
    "for idx, (sku, cat) in enumerate(skus, 1):\n",
    "    g = daily[(daily[sku_col]==sku) & (daily[cat_col]==cat)].sort_values(\"day\").copy()\n",
    "    stats_row = sku_price_stats[(sku_price_stats[sku_col]==sku) & (sku_price_stats[cat_col]==cat)]\n",
    "    if stats_row.empty:\n",
    "        continue\n",
    "\n",
    "    latest_p = float(stats_row[\"latest_price\"])\n",
    "    min_p = float(stats_row[\"min_price\"])\n",
    "    max_p = float(stats_row[\"max_price\"])\n",
    "    n_days = int(stats_row[\"n_days\"])\n",
    "    unit_cost = stats_row[\"unit_cost\"].values[0] if \"unit_cost\" in stats_row.columns else np.nan\n",
    "    if not np.isfinite(unit_cost):\n",
    "        unit_cost = latest_p * COST_FRACTION_OF_PRICE\n",
    "\n",
    "    if idx % PRINT_EVERY_SKU == 0 or idx == 1:\n",
    "        log(f\"[{idx}/{len(skus)}] SKU={sku} | Cat={cat} | n_days={n_days} | latest={latest_p:.2f}\")\n",
    "\n",
    "    # SARIMAX fit + validation (once per SKU)\n",
    "    sarimax_res, sarimax_val = sarimax_fit_and_validate(g)\n",
    "    if sarimax_res is None:\n",
    "        log(f\"  SARIMAX: not enough data or fit failed for {sku}\")\n",
    "\n",
    "    # Skip if too little data for price models\n",
    "    if n_days < MIN_DAILY_ROWS_SKU:\n",
    "        if sarimax_res is not None:\n",
    "            fc = sarimax_forecast_for_price_with_model(sarimax_res, g, latest_p)\n",
    "            if fc is not None:\n",
    "                records.append({\n",
    "                    \"sku\": sku, \"category\": cat, \"model\": \"SARIMAX_baseline\",\n",
    "                    \"optimal_price\": round(latest_p,2),\n",
    "                    \"expected_30d_qty\": round(float(fc.sum()),2),\n",
    "                    \"expected_30d_profit\": round(float((latest_p - unit_cost) * fc.sum()),2),\n",
    "                    \"latest_price\": latest_p,\n",
    "                    **sarimax_val,\n",
    "                    \"note\": \"Baseline at current price\"\n",
    "                })\n",
    "        records.append({\n",
    "            \"sku\": sku, \"category\": cat, \"model\": \"insufficient_data\",\n",
    "            \"optimal_price\": np.nan, \"expected_30d_qty\": np.nan, \"expected_30d_profit\": np.nan,\n",
    "            \"latest_price\": latest_p, **sarimax_val, \"note\": f\"Only {n_days} daily rows\"\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    # Holdout split for price->qty models\n",
    "    def time_split_local(g_local):\n",
    "        gtr, gte = time_split(g_local)\n",
    "        return gtr, gte\n",
    "\n",
    "    g_train, g_test = time_split_local(g)\n",
    "    avg_ctrls_train = {c: float(g_train[c].mean()) for c in CTRL_COLS}\n",
    "\n",
    "    # Fit price->qty models on train\n",
    "    fitted = []\n",
    "    for fitter in (fit_loglog, fit_loglog_quad, fit_spline, fit_rf):\n",
    "        try:\n",
    "            res_m = fitter(g_train)\n",
    "            if res_m is not None:\n",
    "                fitted.append(res_m)\n",
    "                log(f\"  Fitted {res_m[0]} on train (rows={len(g_train)})\")\n",
    "        except Exception as e:\n",
    "            log(f\"  Fit error {fitter.__name__}: {e}\")\n",
    "\n",
    "    # Validate models on test\n",
    "    model_metrics = {}\n",
    "    for m in fitted:\n",
    "        try:\n",
    "            metrics = eval_holdout(m, g_train, g_test)\n",
    "            model_metrics[m[0]] = metrics\n",
    "            if metrics:\n",
    "                log(f\"  Holdout {m[0]}: {metrics}\")\n",
    "        except Exception as e:\n",
    "            log(f\"  Holdout error {m[0]}: {e}\")\n",
    "\n",
    "    # Candidate price grid\n",
    "    price_grid = make_price_grid(latest_p, min_p, max_p, pct=PRICE_BOUND_PCT, n=N_PRICE_GRID)\n",
    "\n",
    "    # For each model: evaluate candidate prices via SARIMAX (if fitted)\n",
    "    for m in fitted:\n",
    "        best_price = None\n",
    "        best_profit = -np.inf\n",
    "        best_qty_sum = 0.0\n",
    "\n",
    "        if sarimax_res is None:\n",
    "            # fallback quick eval: use price->qty model directly (avg daily * 30)\n",
    "            try:\n",
    "                grid_frame = build_grid_frame(price_grid, avg_ctrls_train, m[0])\n",
    "                q_daily = predict_qty_from_model(m, grid_frame)\n",
    "                q_30d = np.maximum(q_daily * FORECAST_HORIZON, 0.0)\n",
    "                profit_30d = (price_grid - unit_cost) * q_30d\n",
    "                j = int(np.argmax(profit_30d))\n",
    "                best_price, best_profit, best_qty_sum = price_grid[j], profit_30d[j], q_30d[j]\n",
    "            except Exception:\n",
    "                pass\n",
    "        else:\n",
    "            # preferred: SARIMAX 30d forecast at each candidate price\n",
    "            for p in price_grid:\n",
    "                fc = sarimax_forecast_for_price_with_model(sarimax_res, g, p)\n",
    "                if fc is None:\n",
    "                    continue\n",
    "                qty_30d = float(fc.sum())\n",
    "                profit_30d = (p - unit_cost) * qty_30d\n",
    "                if profit_30d > best_profit:\n",
    "                    best_profit = profit_30d\n",
    "                    best_price = p\n",
    "                    best_qty_sum = qty_30d\n",
    "\n",
    "        # ensure metric keys exist to avoid KeyError later\n",
    "        row_metrics = {\n",
    "            \"r2_log\": np.nan, \"r2_qty\": np.nan, \"mae_qty\": np.nan,\n",
    "            \"sarimax_rmse\": np.nan, \"sarimax_mape\": np.nan\n",
    "        }\n",
    "        row_metrics.update(model_metrics.get(m[0], {}))\n",
    "        row_metrics.update(sarimax_val if sarimax_val else {})\n",
    "\n",
    "        # record\n",
    "        records.append({\n",
    "            \"sku\": sku, \"category\": cat, \"model\": m[0],\n",
    "            \"optimal_price\": round(float(best_price), 2) if best_price is not None else np.nan,\n",
    "            \"expected_30d_qty\": round(float(best_qty_sum), 2) if best_price is not None else np.nan,\n",
    "            \"expected_30d_profit\": round(float(best_profit), 2) if best_price is not None else np.nan,\n",
    "            \"latest_price\": latest_p,\n",
    "            **row_metrics,\n",
    "            \"note\": \"SARIMAX-evaluated\" if sarimax_res is not None else \"Fallback: direct model\"\n",
    "        })\n",
    "\n",
    "    # Also add a SARIMAX baseline (latest price) if we have SARIMAX\n",
    "    if sarimax_res is not None:\n",
    "        fc = sarimax_forecast_for_price_with_model(sarimax_res, g, latest_p)\n",
    "        if fc is not None:\n",
    "            base_metrics = {\"sarimax_rmse\": np.nan, \"sarimax_mape\": np.nan}\n",
    "            base_metrics.update(sarimax_val if sarimax_val else {})\n",
    "            records.append({\n",
    "                \"sku\": sku, \"category\": cat, \"model\": \"SARIMAX_baseline\",\n",
    "                \"optimal_price\": round(latest_p,2),\n",
    "                \"expected_30d_qty\": round(float(fc.sum()),2),\n",
    "                \"expected_30d_profit\": round(float((latest_p - unit_cost) * fc.sum()),2),\n",
    "                \"latest_price\": latest_p,\n",
    "                **base_metrics,\n",
    "                \"note\": \"Baseline at current price\"\n",
    "            })\n",
    "\n",
    "log(f\"Per-SKU loop done in {time.time()-t0:.1f}s\")\n",
    "\n",
    "# -------------------------------\n",
    "# 6) OUTPUTS (ALL + BEST)\n",
    "# -------------------------------\n",
    "out = pd.DataFrame.from_records(records)\n",
    "if out.empty:\n",
    "    log(\"No results produced. Check data and thresholds.\")\n",
    "else:\n",
    "    # ensure metric columns exist to avoid KeyErrors later\n",
    "    for col in [\"r2_log\",\"r2_qty\",\"mae_qty\",\"sarimax_rmse\",\"sarimax_mape\"]:\n",
    "        if col not in out.columns:\n",
    "            out[col] = np.nan\n",
    "\n",
    "    best_by_sku = (\n",
    "        out.dropna(subset=[\"expected_30d_profit\"])\n",
    "           .sort_values([\"sku\",\"category\",\"expected_30d_profit\"], ascending=[True, True, False])\n",
    "           .groupby([\"sku\",\"category\"])\n",
    "           .head(1)\n",
    "    )\n",
    "\n",
    "    out_all = \"sku_price_optimization_all_models_with_sarimax_debug.csv\"\n",
    "    out_best = \"sku_price_optimization_best_model_per_sku_debug.csv\"\n",
    "    out.to_csv(out_all, index=False)\n",
    "    best_by_sku.to_csv(out_best, index=False)\n",
    "\n",
    "    log(\"\\n=== BEST MODEL PER SKU (top 10 by profit) ===\")\n",
    "    try:\n",
    "        print(best_by_sku.sort_values(\"expected_30d_profit\", ascending=False).head(10).to_string(index=False))\n",
    "    except Exception:\n",
    "        print(best_by_sku.head(10))\n",
    "\n",
    "    log(f\"\\nSaved ALL model results: {os.path.abspath(out_all)}\")\n",
    "    log(f\"Saved BEST per SKU:      {os.path.abspath(out_best)}\")\n",
    "\n",
    "# ===============================\n",
    "# 7) ONE-LINE-PER-SKU SUMMARY\n",
    "# ===============================\n",
    "if out.empty:\n",
    "    log(\"Skipping one-line summary because results are empty.\")\n",
    "else:\n",
    "    # Base per-SKU aggregates from daily\n",
    "    sku_agg = (\n",
    "        daily.sort_values(\"day\")\n",
    "             .groupby([sku_col, cat_col])\n",
    "             .agg(\n",
    "                 total_qty=(\"qty\",\"sum\"),\n",
    "                 total_revenue=(\"revenue\",\"sum\"),\n",
    "                 min_price=(\"price\",\"min\"),\n",
    "                 max_price=(\"price\",\"max\"),\n",
    "                 avg_price=(\"price\",\"mean\"),\n",
    "                 median_price=(\"price\",\"median\"),\n",
    "                 n_days=(\"day\",\"nunique\"),\n",
    "                 first_day=(\"day\",\"min\"),\n",
    "                 last_day=(\"day\",\"max\"),\n",
    "                 promo_days=(\"promo_flag\",\"sum\")\n",
    "             )\n",
    "             .reset_index()\n",
    "    )\n",
    "    sku_agg[\"promo_days_pct\"] = (sku_agg[\"promo_days\"] / sku_agg[\"n_days\"]).fillna(0).round(4)\n",
    "\n",
    "    # active months count\n",
    "    months = daily.assign(ym=daily[\"day\"].dt.to_period(\"M\"))\n",
    "    active_months = months.groupby([sku_col, cat_col])[\"ym\"].nunique().reset_index().rename(columns={\"ym\":\"active_months\"})\n",
    "    sku_agg = sku_agg.merge(active_months, on=[sku_col, cat_col], how=\"left\")\n",
    "\n",
    "    # best historical price by qty (price on the day with max qty)\n",
    "    idx_max_qty_day = daily.groupby([sku_col, cat_col])[\"qty\"].idxmax()\n",
    "    best_hist = daily.loc[idx_max_qty_day, [sku_col, cat_col, \"price\", \"qty\"]].rename(\n",
    "        columns={\"price\":\"best_hist_price_by_qty\", \"qty\":\"best_hist_qty\"}\n",
    "    )\n",
    "\n",
    "    # Merge with price stats (latest/min/max)\n",
    "    sku_one_line = (sku_agg\n",
    "        .merge(sku_price_stats[[sku_col, cat_col, \"latest_price\", \"min_price\", \"max_price\"]].drop_duplicates(),\n",
    "               on=[sku_col, cat_col], how=\"left\", suffixes=(\"\",\"_dup\"))\n",
    "        .merge(best_hist, on=[sku_col, cat_col], how=\"left\")\n",
    "    )\n",
    "\n",
    "    # Best model per SKU (ensure all metric columns exist)\n",
    "    for col in [\"r2_log\",\"r2_qty\",\"mae_qty\",\"sarimax_rmse\",\"sarimax_mape\",\n",
    "                \"expected_30d_profit\",\"expected_30d_qty\",\"optimal_price\"]:\n",
    "        if col not in out.columns:\n",
    "            out[col] = np.nan\n",
    "\n",
    "    if not out.empty and \"expected_30d_profit\" in out.columns:\n",
    "        best_model_cols = [\"sku\",\"category\",\"model\",\"optimal_price\",\"expected_30d_qty\",\"expected_30d_profit\",\n",
    "                           \"r2_log\",\"r2_qty\",\"mae_qty\",\"sarimax_rmse\",\"sarimax_mape\"]\n",
    "        best_model_cols = [c for c in best_model_cols if c in out.columns]\n",
    "        best_model = (out.dropna(subset=[\"expected_30d_profit\"])\n",
    "                        .sort_values([\"sku\",\"category\",\"expected_30d_profit\"], ascending=[True, True, False])\n",
    "                        .groupby([\"sku\",\"category\"])\n",
    "                        .head(1)[best_model_cols]\n",
    "                     )\n",
    "        rename_map = {\n",
    "            \"model\":\"best_model_name\",\n",
    "            \"optimal_price\":\"best_model_optimal_price\",\n",
    "            \"expected_30d_qty\":\"best_model_expected_30d_qty\",\n",
    "            \"expected_30d_profit\":\"best_model_expected_30d_profit\"\n",
    "        }\n",
    "        best_model = best_model.rename(columns={k:v for k,v in rename_map.items() if k in best_model.columns})\n",
    "        sku_one_line = sku_one_line.merge(best_model, on=[sku_col, cat_col], how=\"left\")\n",
    "\n",
    "    # Column order (only keep those present)\n",
    "    cols_order = [\n",
    "        sku_col, cat_col, \"total_qty\", \"total_revenue\",\n",
    "        \"min_price\", \"max_price\", \"avg_price\", \"median_price\", \"latest_price\",\n",
    "        \"best_hist_price_by_qty\", \"best_hist_qty\",\n",
    "        \"n_days\", \"active_months\", \"first_day\", \"last_day\", \"promo_days\", \"promo_days_pct\",\n",
    "        \"best_model_name\", \"best_model_optimal_price\",\n",
    "        \"best_model_expected_30d_qty\", \"best_model_expected_30d_profit\",\n",
    "        \"r2_log\", \"r2_qty\", \"mae_qty\", \"sarimax_rmse\", \"sarimax_mape\"\n",
    "    ]\n",
    "    cols_order = [c for c in cols_order if c in sku_one_line.columns]\n",
    "    sku_one_line = sku_one_line[cols_order]\n",
    "\n",
    "    # Save summary\n",
    "    sku_summary_path = \"sku_one_line_summary.csv\"\n",
    "    sku_one_line.to_csv(sku_summary_path, index=False)\n",
    "\n",
    "    log(f\"\\nSaved ONE-LINE summary per SKU: {os.path.abspath(sku_summary_path)}\")\n",
    "    try:\n",
    "        print(sku_one_line.head(10).to_string(index=False))\n",
    "    except Exception:\n",
    "        print(sku_one_line.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6477c6d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de80a782",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
